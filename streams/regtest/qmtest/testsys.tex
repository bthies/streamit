\documentclass[11pt]{article}

\usepackage{booktabs}
% \textsf looks terrible in the postscript world, but quite nice in
% the Computer Modern world.  Using the pslatex package makes it
% look less bad, but still not great.  --dzm
%\usepackage{pslatex}
%\usepackage{palatino}
\usepackage{tabularx}
\usepackage{hyperref}
\usepackage{streamit}

\title{A QMTest-Based Test System for StreamIt}
\author{David Maze\\revised and expanded: Allyn Dimock}

\begin{document}

\maketitle
\tableofcontents

\section{Motivation}

The StreamIt compiler is becoming an increasingly complex piece of
software, with several independent back-ends and different compiler
flows.  Our previous test system was based on the Java JUnit test
infrastructure, a system designed for adding light-weight unit tests
to existing Java classes; upon this, we retrofitted a system that
could run a single compiler flow by invoking the Java runtime several
times.  Since then, we have developed a script, \textsf{strc}, to run
the compiler, added standardized sets of options, and added at least
one potential alternate flow to the compiler; we also have a desire to
test the Java library ``compiler'' flow path.

The JUnit infrastructure has become increasingly dated.  Potential
compiler flags were implemented as a 32-bit integer bitfield; when the
flags used for the \textsf{strc} standard optimization sets were
added, 30 of the 32 bits were used.  The old test system could never
support the Java library path without significant coding, and it will
be difficult to add support for either frontend-to-Kopi conversion or
the cluster backend.  It also proves to be very difficult to run the
test system as a set of parallel jobs.

Other people have recommended using the
\href{http://www.qmtest.com/}{QMTest} infrastructure for more involved
system tests.  QMTest is written entirely in Python, but this makes it
easy to add more tests by writing simple Python classes.  Tests can
have dependencies on other tests, and can be run hierarchically; ``run
all of the FM radio tests'' is easy to do, ``run all of the RAW
tests'' is straightforward (but slightly less easy).  QMTest also has
built-in support for running tests in parallel, though customizing it
to our environment may require writing code.

\section{Implementation}

This incarnation of the test system is entirely in Python, with some
additional XML.

\subsection{Directory Layout}

The default setup for QMTest is to have a ``test
database'' consisting of a set of directories with embedded XML files;
a \textsf{QMTest} directory at the top level contains additional
control information.  The \textsf{build-qmtest.py} script reads a
control file, then builds a directory tree that QMTest will notice.
By default this builds a tree parallel to the StreamIt \textsf{apps}
directory named \textsf{apps.qms}, copying in files from the main tree
as needed.

The script searches for files named \textsf{benchmark.xml} and reads
those.  In particular, the relevant XML file must have one or more
\verb|<impl>|s for the ``StreamIt'' language; all files listed there
are copied into all of the child directories of the matching QMTest
directory.  Implementations are named by the ``id'' attribute of the
corresponding XML tag, or as ``impl1'', ``impl2'', and so on if there is 
no ``id'' attribute.

The QMTest name for a particular test then consists of the path to the
XML file, the implementation name, the backend/optimization name, and
then the name of the actual test.  So, for example,
\url{apps.benchmarks.fm.impl1.raw4\_o2.run} would be the QMTest test
that runs the code produced by \texttt{strc -{}-raw 4 -O2} on the only
implementation in \textsf{apps/benchmarks/fm}.  The test components
are named \textsf{compile}, \textsf{run}, and \textsf{verify}.

\paragraph{Java library.}  Compilation for the Java library uses the
backend name \textsf{library}.  Since \textsf{strc} runs the compiled
program on its own, there is no \textsf{run} stage.

\paragraph{Uniprocessor backend.}  Compilation for the uniprocessor
backend uses the backend name \textsf{uni}, along with
\textsf{uni\_o1} and \textsf{uni\_o2} for optimized compilation.

\paragraph{RAW backend.}  Compilation for a 4x4 RAW chip uses the
backend name \textsf{raw4}, along with \textsf{raw4\_o1} and
\textsf{raw4\_o2} for optimized compilation.  Unlike in the old test
system, the compilation stage does not build the output of the
compiler; we let the execution stage handle this, since the RAW
makefile system will do this on its own.

In all cases, the directory structure matches the name of the test,
with the final component being a file in the target directory.  To be
recognized by QMTest, directory names end with \texttt{.qms}, and test
files with \texttt{.qmt}.

\subsection{Python Scripts}

As mentioned previously, \textsf{build-qmtest.py} generates the
directory layout for QMTest to use.  It does the work of scanning the
directory tree for \textsf{benchmark.xml} files, creating directories,
and creating XML test files.  The Python XML DOM infrastructure is
used to create the various files, which get written out.

These reference QMTest Test classes, which are by and large defined in
\textsf{streamit.py}.  There are three classes here, which describe the
three stages of the StreamIt tests.  Of particular note, the
``verify'' stage is implemented within a single Python function, where
before it was a set of three separate Perl scripts.  Each class
contains an \textsf{arguments} field, which corresponds exactly to the
set of \verb|<argument>| tags that may be declared in the XML test
files.

QMTest also depends on several files being in the \textsf{QMTest}
subdirectory of the top level of the directory tree.  These are the
files \textsf{classes.qmc} and \textsf{configuration}, along with the
\textsf{streamit.py} file which provides the test classes.

\section{Extensibility}

Extensibility was a key concern for this system.  It should prove much
easier to add to the system than to the original system.

\subsection{New Benchmarks}

In the old system, adding benchmarks involves modifying the Java
source for the regtest.  In this system, a new test (or a new
implementation) just involves adding or modifying a
\textsf{benchmark.xml} file; compilation or execution will always
happen, and verification will happen if there is a
\verb|<file class="output">| in the implementation.

\subsection{Fewer Benchmarks}

It is somewhat easy to run QMTest on a set of fewer benchmarks than
all of them.  You can create a test suite using the QMTest GUI, or
reverse-engineer the suite XML file format and programmatically create
a list of tests or suites.  A straightforward future extension will be
to add support for running all of the benchmarks for a particular
backend.  You can already use a truncated path, such as \textsf{qmtest
  run apps.benchmarks.fm} to run all implementations and all
configurations of the FM Radio benchmark.

\subsection{Different Compiler Flags}

The \textsf{regtest.xml} file controls some high-level behavior of the
system, and is read by \textsf{build-qmtest.py} to assemble the
directory tree.  To add or remove compiler flags, add or remove
\verb|<option>| tags from this file.

\subsection{Different Compiler Flows}

Any change in the compiler flow that is hidden within \textsf{strc} is
irrelevant to this test system.  For example, \textsf{strc} handles
the experimental \verb|--tokopi| option to not generate an
intermediate Java file; testing this would just involve adding the
compiler flag to the control file.

For more involved changes, you may need to write a QMTest object.  Add
it to \textsf{streamit.py}.  \textsf{build-qmtest.py} also needs to be
updated to add or create the relevant \texttt{.qmt} file.  For
example, you could use this technique to separately test whether our
generated RAW source code compiles by adding a stage in between the
``compile'' and ``run'' stages.  (This could also be accomplished by
modifying the \textsf{RunStrcTest} Python test class.)

\subsection{Adding Backends}

Adding a backend is probably the most involved change in the current
system.  This requires moderate changes in both \textsf{streamit.py}
and \textsf{build-qmtest.py}, along with adding the backend to
\textsf{regtest.xml}.  In particular, there are places which test the
backend flag for validity, and places which dispatch based on the
backend and assume it is one of the extant backends.

\subsection{Data Files}
The description of the structure of the .xml files is based on looking
at build-qmtest.py and determining what tags it uses.  If it does not
use a tag that is in an example file, then the tag is listed as
optional and shown in the sequence in which it appears on some
randomly-chosen example file.

\subsubsection{regtest.xml}
Format: \verb@regtest (test | option(CDATA) | whichtype(CDATA))+@

All \verb+<test root="DIRNAME">+ indicate directories that might have 
benchmarks.xml files in directory subtrees. (build-qmtest.py processes
this file and does not follow symbolic links in determining the
directory subtree.)

All \verb+<option target="BACKEND">COMMAND_LINE_OPTIONS</option>+
indicate back ends that will be used for all tests.

Current legal values for the target attribute are  ``library'', ``uni'', 
``raw4'', ``simpleC'', or ``cluster''.

All \verb+<whichtype>TESTING_TYPE</whichtype>+ provide keywords indicating a 
subset of tests to be run.
If there are no \verb+<whichtype>+ elements, then all tests found in
{\tt benchmark.xml} files will be considered for running subject to
the constraints in the \verb+<impl>+ elements in the {\tt
benchmark.xml} files .
If there are \verb+<whichtype>+ elements, then only the tests with one
or more matching \verb+<whichtype>+ elements in their \verb+<impl>+
will be considered for running.


\subsubsection{benchmark.xml}
Format:\begin{verbatim}
benchmark 
  (name(CDATA)?, desc(CDATA)?, description(CDATA)?, reference(CDATA)?
   implementations 
     (impl
        (desc(CDATA)? | file(CDATA)+ | whichtype(CDATA)+ | make(CDATA)?)))
       \end{verbatim}%
where {\tt impl} has attributes
\begin{itemize}
\item {\tt regtest} optional.  If value is {\tt skip} then skip
  processing of {\tt file}s in this {\tt impl} 
\item {\tt dir} optional to indicate that {\tt file}s are in a subdirectory.
The value is the relative address of the subdirectory from the
directory containing the benchmark.xml
\item {\tt id} optional, used for naming test directories, for
  reporting.  If absent defaults to ``impl'' plus a sequence number.
\item {\tt lang} Language to use in testing.  build-qmtest.py filters
  out all {\tt impl}s where {\tt lang} $\neq$ {\tt StreamIt}.
\item {\tt iters} Overrides default of running test for 100 iterations of steady state.
\item {\tt own\_output} Name of a file in which the tested program produces
output to verify.  If this is not specified, then the tested program is
expected to produce any output to be verified to {\tt stdout}.
Default output to stdout is stored in the file {\tt output} between the run
phase and the verification phase.
Not supported yet.)
\item {\tt output\_type} Type of output.  Legitimate values are {\tt
  ASCII}, {\tt int}, and {\tt float} where {\tt int}, and {\tt float}
  refer to 32-bit integers and 32-bit floats, and ASCII is a series of
  lines with one formatted integer or floating point value per line.
(Not supported yet.)
\item {\tt compile\_time} Time in seconds to override default compile time of 20 minutes.
\item {\tt run\_time} Time in seconds to override default run time of 20 minutes.
In case of compile-and-go situation like library compiler, use {\tt compile\_time}, not {\tt run\_time}.
\end{itemize}

{\tt make} names a make file, which is run before processing {\tt file}s.
Since the text is just processed by ``make -f ...'', you can put targets, 
switches, ... on this line as well.

{\tt file} has two attributes: {\tt class} and {\tt relative}.  
If the value of {\tt class}
is "source" or if class is missing then the file name is of a source
file. A ``source'' file is considered to be an input to the compiler.
 If {\tt class} is "output" then the file name is the is expected
output. A ``output'' file is input to the verification phase of testing.
If there is no "output" file, then the source is compiled and
run, but any output from the run is considered to pass the test.
If the {\tt class} is ``data'' (or any other string) then the file is just 
copied to the test directory. 

The {\tt relative} attribute of a {\tt file}, if ``true'', will cause the 
file to not be copied to the test directory, but rather to the directory
specified in the file path.  If the directory is relative then the path will
be copied on a path relative to the test directory.  This is useful for
benchmarks that read from (for instance) {\tt ../inputs/data.bin}. 

%% {\tt proc\_output} is a test field containing a {\tt bash} command to be
%% run between the run phase and the verification phase.  It is meant to be
%% used to process output into a form that the verification phase can understand.
%% For instance, if your program produces binary output, but the verification
%% phase expacts ASCII output, you can use {\tt proc\_output} to perform the
%% conversion.
%% (Extension:  in the future we should introduce an attribute {\tt canned\_transform="float"} or {\tt canned\_transform="int"}.  But not there yet.)

A {\tt whichtype} has a text field indicating that the test is to be run
if the text matches one of the whichtypes from {\tt benchmarks.xml}.
This is used to categorize tests for inclusion into various testing
scenarios: regression testing, testing before a CVS commit, ... 
If either {\tt benchmarks.xml} does not specify any {\tt whichtype}s
or the {\tt impl} does not specify any {\tt whichtype}s then the test
will be run.

\subsubsection{classes.qmc}
Points QM at some overridden or extended classes in {\tt streamit.py}.

\subsubsection{configuration}
Tells QM what sort of ``database'' of tests we are using:  We are currently
using a directory-structured database where the leaves contain {\tt  xml} 
files giving instructions to QM as well as containing the tests themselves.

\subsubsection{results.qmr}
QM results file in undocumented internal format (where both the
format, and the internal routines to access the elements of the file
may change between versions of QM).  This is the file produced by
``qmtest run''.

\subsubsection{results.xml}
This is a version of the QM results file as processed by ``qmtest report''.
the programs {\tt summarize\_xml\_results} and {\tt rt-results} run off
the contents of this file.
Unfortunately, there is no DTD for this file, and as of QM 2.3, there
was no guarantee from CodeSourcery that the file format would not
change.
However, we expect that the data that we process from the {\tt results.xml}
file will not change format.

\subsubsection{Directories}
{\tt run-reg-tests.py} builds the following directory structure:
A root directory with a name derived from the date and time that the
regression test was started.
This directory contains log files from pre- and post-processing
phases, and two subdirectories.

The ``rt'' subdirectory is where the files that describe the
regression test to the RT ticket-management system will reside after
testing. (There is an extension to RT to allow browsing these files.)

The ``streams'' subdirectory is a fresh {\tt cvs} checkout of the
entire StreamIt source tree.  This checkout includes some of the
programs used in regression testing.

In the ``streams'' subdirectory, regression testing builds two new
subdirectories and two files:  The files ``results.qmr'' and
``results.xml'' are described above.
The new subdirectories are ``QMTest'' and ``apps.qms''.

The ``streams/QMtest'' subdirectory contains the files needed to
tailor QM to our needs:  ``configuration'', ``classes.qmc'', and the
source and compiled version of ``streamit.py'' which contains extension
classes adapting the QM test framework to out needs.

The ``streams/apps.qms'' subdirectory is the root of the test
database.  Consider a path under ``apps.qms'' to be similar to the
path under ``streams/apps'' to a benchmark, except that ``.qms'' is
added to the name of each subdirectory.  There is a formatting
difference: QM requires that all letters are lowercase and that
hyphens and other odd characters in directory and file names be
removed.

At the leaf of a  {\tt streams/apps.qms} directory, find: 
\begin{itemize}
\item a {\tt .str} file to test
\item optionally, an output file to verify the results of running the 
compiled program,
\item {\tt compile.qmt}, {\tt run.qmt}, and {\tt verify.qmt}
containing instructions to QM in {\tt xml} format.
This includes the name of the class that performs the test, the test
name, any arguments needed to run the test, and any prerequisites
needed to order tests correctly.
\end{itemize}
{\tt build-qmtest.py} creates the {\tt apps.qms} directory structure and
the {\tt .qmt} files based on {\tt impl} fields in a {\tt benchmark.xml}
file and {\tt option}s in the {\tt regtest.xml} file.

Recently an evil hack has been added so that {\tt run-reg-tests.py} builds 
\url{/home/public/streamit/latest\_javadoc} and \url{/home/public/streamit/latest\_doccheck}.
This behavior (going outside of the subdirectory structure) should be on a switch
and the switch should be off by default!

\subsection{Programs and Control Flow}

\subsubsection{streamit.py}
Extends / overrides some QM methods, and contains a few utility routines.

The current version includes its own version of ``TimedExecutable''
since that was not supplied in QMTest 2.0 (It appears in later
versions).  
%% This version of TimedExecutable does not work with
%% ``process targets'':
%% \begin{verbatim}
%% qmtest create-target -a processes=4 p4 process_target.ProcessTarget
%% qmtest run
%% \end{verbatim}
%% will occasionally come up withe an EACCESS error (Error 13) when the
%% process target performs some exec functions before TimedExecutable can
%% perform a setpgid.

%% The three Run methods: RunStrcTest, which compiles; RunProgramTest,
%% which executes; and CompareResultsTest, which verifies output
%% all currently require that a chdir is made to the proper test
%% directory.
%% This makes it impossible to use a thread target
%% \begin{verbatim}
%% qmtest run -j4
%% \end{verbatim}
%% since threads and chdir do not interact well.
%% A common symptom of attempting this is verification errors trying to
%% verify one test's output against another test's expected output file.

\subsubsection{run-reg-tests.py}
\begin{itemize}
\item check out clean directory:
 Under directory named in global variable {\tt regtest\_root} check
 out a fresh copy of streams from CVS, build the compiler (src), the C
 library (library/c) and the raw toolchain (misc/raw),
\item use streamitqm / run-reg-tests.py to convert from our .xml files to QM
  files,
\item run QM,  ({\tt qmtest run} followed by  {\tt qmtest report})
\item run {\tt summarize\_xml\_results} to create summary report and
  mail it to all. 
\item run rt-results to build files usable by RT for detailed
  reporting of the results,
\item and set the directory built by this run to be the ``latest'' results.
\end{itemize}

\subsubsection{streamitqm}
Shell script used to get from run-reg-tests.py to 
\url{./regtest/qmtest/build-qmtest.py}
plus some file handling.  {\tt run-reg-tests.py} expects this file to be in 
\url{$STREAMIT\_HOME/regtest/qmtest/streamitqm}. % $

\subsubsection{build-qmtest.py}
Executable, invoked with 1 parameter: the complete path and file name for the 
regtest.xml file.  streamitqm expects this file to be in \url{./regtest/qmtest}.
Creates files {\tt compile.qmt}, {\tt run.qmt}, and {\tt verify.qmt}
for each combination of backend and options.

\subsubsection{summarize\_xml\_results}
Executable, invoked from run-reg-tests.py to produce a brief report based on
the current QMTest results.xml file, and differences from the ``latest''
QMTest \url{results.xml} file if one exists. The first parameter is the path to
the current QMTest \url{results.xml} file, the second parameter is the path to
the latest QMtest \url{results.xml} file. 
This program does not work directly on the {\tt .xml} files, but
rather on a summary of the test outcomes produced by {\tt  status\_summary.xsl}.

{\tt summarize\_xml\_results} is an older version that attempted to
work directlry off the {\tt .xml} files.  It proved too slow.


\subsubsection{rt-results}
Executable, invoked with 2 parameters: path to the \url{results.xml} file (defaults
to \url{results.xml} if not given), 
and path to place to put reports (defaults to ``.'' if not given).
Creates files 
{\tt Summary}, 
{\tt RegtestItem}, 
{\tt listing.html},
and a {\tt .html} file per test.

These files are used to show the latest QMtest results on the 
RT ticket tracking system. 
 The files are not very readable on their own.

\subsection{pbs\_regtest\_nightly}
This is a script to run {\tt run-reg-tests.py} as a PBS job.
See \url{ttps://www2.cag.csail.mit.edu/cagwiki/index.php?title=Batch_Queuing}
for notes on using PBS.

There should be a {\tt cron} job someplace that looks a bit like
{\small{
\begin{verbatim}
[cagfarm-49:~] crontab -l
...
# run regression test daily at 8:00PM
00 20 * * * /usr/uns/bin/qsub /u/dimock/streams/regtest/qmtest/pbs_regtest_nightly
\end{verbatim}
}}
\subsection{delete\_old\_regtests}
The regression tests generate a massive amount of data: 
a full checkout of the streams directory, with portions of the {\tt apps} 
directories duplicated once per test type (back end + flags), plus all 
{\tt stdout} and {\tt stderr} from each test. ({\tt stdout} of test occurrs
in test directory.  Both {\tt stdout} and {\tt stderr} are captured in 
\url{/home/bits8/streamit/regtest/[date]/streams/results.qmr}
and \url{/home/bits8/streamit/regtest/[date]/streams/results.xml}

{\tt delete\_old\_regtests} is a script that copies a bzip'd regtest.xml
into \url{/home/bits8/streamit/old_regtest_results} and deletes the
corresponding regtest directory.
It leaves 8 days of complete regtest results.  If
\url{old_regtest_results} exceeds 100Mb, then it also removes every
second file in \url{old_regtest_results}, resulting in exponential
decay in in the stored regression test results over time.

{\tt delete\_old\_regtests} should be run on a regular basis, for
instance:
{\small{
\begin{verbatim}
[cagfarm-49:~] crontab -l
...
# run regression test daily at 8:00PM
00 20 * * * /usr/uns/bin/qsub /u/dimock/streams/regtest/qmtest/delete_old_regtests
\end{verbatim}
}}

\subsection{status\_files.xsl and files\_check\_timout.pl}
{\tt status\_files.xsl} breaks a {\tt results.xml} file up into
individual files for each test with an outcome other than
``UNTESTED''. 
{\tt files\_check\_timout.pl \*.xml} takes a directory of {\tt .xml}
files produced from  {\tt results.xml} by {\tt status\_files.xsl} and
reports the running time of each file that exited with a
signal. (Running out of time on compiling or runnign results in a test
being killed with signal 9.)  The user can then decide as to whether
to give a test more compile or run time.
More time can be allowed by changing the {\tt compile\_time}
or {\tt run\_time} attribute values on an {\tt impl} line in a 
{\tt benchmark.xml} file.

\section{Running Tests manually}

There are times when you may want to set up a script that runs some
subset of tests, or you may want to run a few tests from your
terminal.
Here is what to do.

\begin{itemize}
\item Set up a file based on \url{$STREAMIT\_HOME/regtest/qmtest/regtest.xml} %$
specifying the back ends, switch combinations, and the roots of a forest of
directories under {\tt apps/} that you wish to test.

All benchmarks with a {\tt benchmark.xml} file under a specified test root 
will be tested.

In the following, assume that the file you create is called {\url my\_tests.xml}

\item Type the following commands:
{\small{
\begin{verbatim}
 cd $STREAMIT_HOME
               # remove evidence of previous regtest
 ./regtest/qmtest/streamitqm clean
               # create directories and xml for regtest
 ./regtest/qmtest/streamitqm setup my_tests.xml
               # tell QM where its control files are
 setenv QMTEST_CLASS_PATH $STREAMIT_HOME/QMTest
               # run regression test
 qmtest run
\end{verbatim}
}}
This will start a regtest running with your local copy of the source
tree and outputting the results to the screen.  The applications and
compile options being tested are stored under
\url{$STREAMIT_HOME/apps.qms}. % $

\item You may be able to get the information that you want directly from the
{\tt qmtest} output.

If the output from {\tt qmtest} is too verbose, you can see a summary
report similar to that from the nightly regression test using the commands:
{\small{
\begin{verbatim}
 qmtest report  -o ./results.xml ./results.qmr 
 xsltproc -o summary_tmp  --novalid ./regtest/qmtest/status_summary.xsl \
    ./results.xml
 ./regtest/qmtest/status_summary.pl summary_tmp > summary.txt
\end{verbatim}
}}

If you want to compare the results of your test with the latest
nightly regression test, try:
{\small{
\begin{verbatim}
 qmtest report -o ./results.xml ./results.qmr 
 xsltproc -o summary_tmp  --novalid ./regtest/qmtest/status_summary.xsl \
    ./results.xml
 xsltproc -o summary_tmp2  --novalid ./regtest/qmtest/status_summary.xsl \
    /home/bits8/streamit/regtest/latest/streams/results.xml
 ./regtest/qmtest/status_summary.pl summary_tmp summary_tmp2 > summary.txt
\end{verbatim}
}}
\end{itemize}
% note to self: create-target format
% qmtest create-target -a processes=5 p5 process_target.ProcessTarget p
\end{document}
